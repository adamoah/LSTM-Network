{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GithubRepoCode.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7WE4ESRk2T6Y+kWZF8PUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamoah/LSTM-Network/blob/main/GithubRepoCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3STjzxM1IPT"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import phik\n",
        "\n",
        "\n",
        "#RMSE loss function\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "#Set seed for random number generator\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "#Data preparation\n",
        "Chicago_df = pd.read_csv('Chicago-Network-Data.csv')\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "for i in range(8, 487):\n",
        "  x.append([Chicago_df.iloc[i, 0], Chicago_df.iloc[i, 1]])\n",
        "  y.append(Chicago_df.iloc[i, 2])\n",
        "\n",
        "input_x = np.array(x)\n",
        "output_y = np.array(y)\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "input_x = scaler.fit_transform(input_x)\n",
        "output_y = output_y.reshape(-1, 1)\n",
        "output_y = scaler.fit_transform(output_y)\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(input_x, output_y, test_size=0.2, shuffle=False)\n",
        "\n",
        "trainX = trainX.reshape(len(trainX), 1, trainX.shape[1])\n",
        "testX = testX.reshape(len(testX), 1, testX.shape[1])\n",
        "\n",
        "\n",
        "#initialize model\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(4))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss=root_mean_squared_error, optimizer='adam')\n",
        "print('model is ready to use')\n",
        "\n",
        "#run model training\n",
        "history = model.fit(trainX, trainY, batch_size=1, epochs=100, verbose=2, validation_data=(testX, testY))\n",
        "\n",
        "\n",
        "#print RMSE of the model for train and test\n",
        "train_loss=model.evaluate(trainX, trainY)\n",
        "test_loss = model.evaluate(testX, testY)\n",
        "print('Train Loss: ', train_loss)\n",
        "print('Test Loss: ', test_loss)\n",
        "\n",
        "#graph model loss\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "ax.plot(history.history['loss'])\n",
        "ax.plot(history.history['val_loss'])\n",
        "ax.set_title('Chicago Model Loss')\n",
        "ax.set_xlabel('epochs')\n",
        "ax.set_ylabel('loss')\n",
        "ax.legend(['train', 'test'], loc='upper right')\n",
        "\n",
        "\n",
        "#get model predictions for train/test data\n",
        "trainPredictions = model.predict(trainX)\n",
        "testPredictions = model.predict(testX)\n",
        "\n",
        "#invert predictions and observed data to be in original scale\n",
        "output_y = scaler.inverse_transform(output_y)\n",
        "trainPredictions = scaler.inverse_transform(trainPredictions)\n",
        "testPredictions = scaler.inverse_transform(testPredictions)\n",
        "\n",
        "#shift train predictions to match observed data\n",
        "trainPredictionsPlot = np.empty_like(output_y)\n",
        "trainPredictionsPlot[:, :] = np.nan\n",
        "trainPredictionsPlot[0:len(trainPredictions), :] = trainPredictions\n",
        "\n",
        "#shift test predictions to match observed data\n",
        "testPredictionsPlot = np.empty_like(output_y)\n",
        "testPredictionsPlot[:, :] = np.nan\n",
        "testPredictionsPlot[len(trainPredictions):len(output_y), :] = testPredictions\n",
        "\n",
        "#plot predictions\n",
        "fig2, ax2 = plt.subplots(1, 1, figsize=(9, 8))\n",
        "ax2.plot(output_y)\n",
        "ax2.plot(trainPredictionsPlot)\n",
        "ax2.plot(testPredictionsPlot)\n",
        "ax2.set_title('Chicago Model Predictions')\n",
        "ax2.set_ylabel('kg/kg')\n",
        "ax2.set_xlabel('Months')\n",
        "ax2.legend(['Observed', 'Train', 'Test'], loc='upper right')\n",
        "\n",
        "#compile all data and convert into pandas df\n",
        "predictions = np.concatenate((trainPredictions, testPredictions))\n",
        "data = []\n",
        "for i in range(0, len(x)):\n",
        "  data.append([x[i][0], x[i][1], output_y.flatten()[i], predictions.flatten()[i]])\n",
        "data = np.array(data)\n",
        "df = pd.DataFrame(data, columns=['Temperature', 'Humidity', 'Observed Ozone', 'Predicted Ozone'])\n",
        "\n",
        "#display phik correlation matrix of all columns\n",
        "phik_overview = df.phik_matrix()\n",
        "phik_overview\n",
        "\n",
        "#save model\n",
        "model.save('Chicago_Dataset')\n",
        "\n",
        "#reset model\n",
        "K.clear_session()\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform\n",
        "\n",
        "#set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "#data parsing function\n",
        "def data():\n",
        "\n",
        "  Chicago_df = pd.read_csv('Chicago-Network-Data.csv')\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(8, 487):\n",
        "    x.append([Chicago_df.iloc[i, 0], Chicago_df.iloc[i, 1]])\n",
        "    y.append(Chicago_df.iloc[i, 2])\n",
        "  input_x = np.array(x)\n",
        "  output_y = np.array(y)\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  input_x = scaler.fit_transform(input_x)\n",
        "  output_y = output_y.reshape(-1, 1)\n",
        "  output_y = scaler.fit_transform(output_y)\n",
        "  trainX, testX, trainY, testY = train_test_split(input_x, output_y, test_size=0.2, shuffle=False)\n",
        "  trainX = trainX.reshape(len(trainX), 1, trainX.shape[1])\n",
        "  testX = testX.reshape(len(testX), 1, testX.shape[1])\n",
        "\n",
        "  return trainX, trainY, testX, testY\n",
        "\n",
        "\n",
        "#initilize model function and return necessary hyperas components\n",
        "def create_model(trainX, trainY, testX, testY):\n",
        "\n",
        "  layers = {{choice[(1, 2, 3)]}}\n",
        "  model = Sequential()\n",
        "\n",
        "  if layers == 1:\n",
        "    model.add(LSTM({{choice([4, 8, 16, 32, 64])}}, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
        "  elif layers == 2:\n",
        "    model.add(LSTM({{choice([4, 8, 16, 32, 64])}}, input_shape=(trainX.shape[1], trainX.shape[2])), return_sequences=True)\n",
        "    model.add(LSTM({{choice([4, 8, 16, 32, 64])}})\n",
        "  else:\n",
        "    model.add(LSTM({{choice([4, 8, 16, 32, 64])}}, input_shape=(trainX.shape[1], trainX.shape[2])), return_sequences=True)\n",
        "    model.add(LSTM({{choice([4, 8, 16, 32, 64])}})\n",
        "    model.add(LSTM({{choice([4, 8, 16, 32, 64])}})\n",
        "\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mse', optimizer={{choice(['adam', 'sgd', 'rmsprop'])}})\n",
        "  model.fit(trainX, trainY, batch_size={{choice([1, 2, 3])}}, epochs={{choice([50, 100, 150, 200])}}, verbose=0, validation_data=(testX, testY))\n",
        "  score = model.evaluate(testX, testY, verbose=0)\n",
        "  \n",
        "  return {'loss': score, 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "\n",
        "#install pydrive and authorization libraries\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title='HyperasOptimization.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('HyperasOptimization.ipynb')\n",
        "\n",
        "\n",
        "#run optimization\n",
        "best_run, best_model = optim.minimize(model=create_model, data=data, algo=tpe.suggest, max_evals=25, notebook_name='HyperasOptimization', trials=Trials())\n",
        "#display best performing model\n",
        "trainX, trainY, testX, testY = data()\n",
        "print(\"Best performing model evaluation: \")\n",
        "print(best_model.evaluate(testX, testY))\n",
        "print(\"Best model hyperparameters: \")\n",
        "print(best_run)"
      ],
      "metadata": {
        "id": "UGZYFkFF1Zm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}